{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf39210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upto_rungs 8\n"
     ]
    }
   ],
   "source": [
    "a = {\n",
    "    'emergency_stop': {\n",
    "        'rung_no': -1,\n",
    "        'next_rung': 3,\n",
    "        'range': '2-3',\n",
    "        'outcoil': ['AL111[1]']\n",
    "    },\n",
    "    'emergency_stop_with_independence_device': {\n",
    "        'rung_no': -1,\n",
    "        'next_rung': 4,\n",
    "        'range': '3-4',\n",
    "        'outcoil': ['AL111[48]']\n",
    "    },\n",
    "    'automatic_stop': {\n",
    "        'rung_no': 4,\n",
    "        'next_rung': 5,\n",
    "        'range': '4-5',\n",
    "        'outcoil': ['AL111[10]']\n",
    "    },\n",
    "    'cycle_stop': {\n",
    "        'rung_no': 5,\n",
    "        'next_rung': 6,\n",
    "        'range': '5-6',\n",
    "        'outcoil': ['AL111[11]']\n",
    "    },\n",
    "    'fault_stop': {\n",
    "        'rung_no': 6,\n",
    "        'next_rung': 7,\n",
    "        'range': '6-7',\n",
    "        'outcoil': ['AL111[31]']\n",
    "    },\n",
    "    'warning': {'rung_no': 7, 'next_rung': 8, 'range': '7-8', 'outcoil': ['AL111[33]']},\n",
    "    'warning_with_independence_device': {\n",
    "        'rung_no': 8,\n",
    "        'next_rung': 9,\n",
    "        'range': '8-9',\n",
    "        'outcoil': ['AL111[49]']\n",
    "    },\n",
    "    'operation_fault': {\n",
    "        'rung_no': -1,\n",
    "        'next_rung': 28,\n",
    "        'range': '9-28',\n",
    "        'outcoil': [\n",
    "            'AL111[41]',\n",
    "            'LB200[0]',\n",
    "            'LB200[1]',\n",
    "            'LB200[2]',\n",
    "            'LB200[9]',\n",
    "            'LB200[19]',\n",
    "            'LB200[20]',\n",
    "            'LB200[21]',\n",
    "            'LB200[22]',\n",
    "            'LB200[23]',\n",
    "            'LB200[24]',\n",
    "            'LB200[29]',\n",
    "            'LB200[30]',\n",
    "            'LB200[31]',\n",
    "            'LB200[39]',\n",
    "            'LB200[40]',\n",
    "            'LB200[41]',\n",
    "            'LB200[42]',\n",
    "            'LB200[49]'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "first_valid_rung = next(\n",
    "    (v.get('rung_no') for k, v in reversed(a.items()) if v.get('rung_no') != -1),\n",
    "    -1  # default if no match found\n",
    ")\n",
    "\n",
    "print(\"upto_rungs\", first_valid_rung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff9d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5436dbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'operand': 'LB200[9]', 'negated': 'false'}, {'operand': 'LB200[19]', 'negated': 'false'}, {'operand': 'LB200[29]', 'negated': 'false'}, {'operand': 'LB200[39]', 'negated': 'false'}, {'operand': 'LB200[49]', 'negated': 'false'}]\n",
      "[{'operand': 'LB200[19]', 'negated': 'false'}, {'operand': 'LB200[29]', 'negated': 'false'}, {'operand': 'LB200[39]', 'negated': 'false'}, {'operand': 'LB200[49]', 'negated': 'false'}]\n",
      "[{'operand': 'LB200[29]', 'negated': 'false'}, {'operand': 'LB200[39]', 'negated': 'false'}, {'operand': 'LB200[49]', 'negated': 'false'}]\n",
      "[{'operand': 'LB200[39]', 'negated': 'false'}, {'operand': 'LB200[49]', 'negated': 'false'}]\n",
      "[{'operand': 'LB200[49]', 'negated': 'false'}]\n"
     ]
    }
   ],
   "source": [
    "a = [[{'operand': 'LB200[9]', 'negated': 'false'}, {'operand': 'LB200[19]', 'negated': 'false'}, {'operand': 'LB200[29]', 'negated': 'false'}, {'operand': 'LB200[39]', 'negated': 'false'}, {'operand': 'LB200[49]', 'negated': 'false'}], [{'operand': 'LB200[19]', 'negated': 'false'}, {'operand': 'LB200[29]', 'negated': 'false'}, {'operand': 'LB200[39]', 'negated': 'false'}, {'operand': 'LB200[49]', 'negated': 'false'}], [{'operand': 'LB200[29]', 'negated': 'false'}, {'operand': 'LB200[39]', 'negated': 'false'}, {'operand': 'LB200[49]', 'negated': 'false'}], [{'operand': 'LB200[39]', 'negated': 'false'}, {'operand': 'LB200[49]', 'negated': 'false'}], [{'operand': 'LB200[49]', 'negated': 'false'}]]\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56842a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_to_half_conversion = {\"ア\":\"ｱ\",\n",
    "\"イ\":\"ｲ\",\n",
    "\"ー\":\"ｰ\",\n",
    "\"ウ\":\"ｳ\",\n",
    "\"エ\":\"ｴ\",\n",
    "\"オ\":\"ｵ\",\n",
    "\"カ\":\"ｶ\",\n",
    "\"キ\":\"ｷ\",\n",
    "\"ク\":\"ｸ\",\n",
    "\"ケ\":\"ｹ\",\n",
    "\"コ\":\"ｺ\",\n",
    "\"サ\":\"ｻ\",\n",
    "\"シ\":\"ｼ\",\n",
    "\"ス\":\"ｽ\",\n",
    "\"セ\":\"ｾ\",\n",
    "\"ソ\":\"ｿ\",\n",
    "\"タ\":\"ﾀ\",\n",
    "\"チ\":\"ﾁ\",\n",
    "\"ツ\":\"ﾂ\",\n",
    "\"テ\":\"ﾃ\",\n",
    "\"ト\":\"ﾄ\",\n",
    "\"ナ\":\"ﾅ\",\n",
    "\"ニ\":\"ﾆ\",\n",
    "\"ヌ\":\"ﾇ\",\n",
    "\"ネ\":\"ﾈ\",\n",
    "\"ノ\":\"ﾉ\",\n",
    "\"ハ\":\"ﾊ\",\n",
    "\"ヒ\":\"ﾋ\",\n",
    "\"フ\":\"ﾌ\",\n",
    "\"ヘ\":\"ﾍ\",\n",
    "\"ホ\":\"ﾎ\",\n",
    "\"マ\":\"ﾏ\",\n",
    "\"ミ\":\"ﾐ\",\n",
    "\"ム\":\"ﾑ\",\n",
    "\"メ\":\"ﾒ\",\n",
    "\"モ\":\"ﾓ\",\n",
    "\"ヤ\":\"ﾔ\",\n",
    "\"ユ\":\"ﾕ\",\n",
    "\"ヨ\":\"ﾖ\",\n",
    "\"ラ\":\"ﾗ\",\n",
    "\"リ\":\"ﾘ\",\n",
    "\"ル\":\"ﾙ\",\n",
    "\"レ\":\"ﾚ\",\n",
    "\"ロ\":\"ﾛ\",\n",
    "\"ワ\":\"ﾜ\",\n",
    "\"ヲ\":\"ｦ\",\n",
    "\"ン\":\"ﾝ\",\n",
    "\"ガ\":\"ｶﾞ\",\n",
    "\"ギ\":\"ｷﾞ\",\n",
    "\"グ\":\"ｸﾞ\",\n",
    "\"ゲ\":\"ｹﾞ\",\n",
    "\"ゴ\":\"ｺﾞ\",\n",
    "\"ザ\":\"ｻﾞ\",\n",
    "\"ジ\":\"ｼﾞ\",\n",
    "\"ズ\":\"ｽﾞ\",\n",
    "\"ゼ\":\"ｾﾞ\",\n",
    "\"ゾ\":\"ｿﾞ\",\n",
    "\"ダ\":\"ﾀﾞ\",\n",
    "\"ヂ\":\"ﾁﾞ\",\n",
    "\"ヅ\":\"ﾂﾞ\",\n",
    "\"デ\":\"ﾃﾞ\",\n",
    "\"ド\":\"ﾄﾞ\",\n",
    "\"バ\":\"ﾊﾞ\",\n",
    "\"ビ\":\"ﾋﾞ\",\n",
    "\"ブ\":\"ﾌﾞ\",\n",
    "\"ベ\":\"ﾍﾞ\",\n",
    "\"ボ\":\"ﾎﾞ\",\n",
    "\"パ\":\"ﾊﾟ\",\n",
    "\"ピ\":\"ﾋﾟ\",\n",
    "\"プ\":\"ﾌﾟ\",\n",
    "\"ペ\":\"ﾍﾟ\",\n",
    "\"ポ\":\"ﾎﾟ\",\n",
    "\"ァ\":\"ｧ\",\n",
    "\"ィ\":\"ｨ\",\n",
    "\"ゥ\":\"ｩ\",\n",
    "\"ェ\":\"ｪ\",\n",
    "\"ォ\":\"ｫ\",\n",
    "\"ャ\":\"ｬ\",\n",
    "\"ュ\":\"ｭ\",\n",
    "\"ョ\":\"ｮ\",\n",
    "\"ッ\":\"ｯ\",\n",
    "\"ヮ\":\"ﾜ\",\n",
    "\"ヴ\":\"ｳﾞ\",\n",
    "\"ヵ\":\"ｶ\",\n",
    "\"ヶ\":\"ｹ\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9daf46fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regex_pattern_check(check_pattern:str, comment_list)->bool:\n",
    "    \n",
    "    ret_flag=0\n",
    "\n",
    "    if check_pattern and isinstance(check_pattern, str) and comment_list and isinstance(comment_list, list):\n",
    "        check_pattern = ''.join(full_to_half_conversion.get(char, char) for char in check_pattern)\n",
    "        for comment in comment_list:\n",
    "            if comment and isinstance(comment, str):\n",
    "                half_width_convert_comment = ''.join(full_to_half_conversion.get(char, char) for char in comment)\n",
    "\n",
    "                print(\"check_pattern\", check_pattern, half_width_convert_comment)\n",
    "                if re.search(check_pattern, half_width_convert_comment):\n",
    "                    ret_flag=1\n",
    "                    break\n",
    "                \n",
    "        if ret_flag==0:\n",
    "            return False\n",
    "        else:\n",
    "            \n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93aa8dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_pattern 緊急停止 非常停止ｲﾝﾀｰﾛｯｸ\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "# total_comment = 'まとめ'\n",
    "emergency_stop_comment = '緊急停止'\n",
    "# independent_device_comment = '独立制御機器'\n",
    "\n",
    "rung_name_str = \"非常停止ｲﾝﾀｰﾛｯｸ\"\n",
    "if regex_pattern_check(emergency_stop_comment, [rung_name_str]):\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_comment = 'まとめ'\n",
    "# emergency_stop_comment = '緊急停止'\n",
    "# independent_device_comment = '独立制御機器'\n",
    "autorun_comment = \"自動運転\"\n",
    "\n",
    "rung_name_str = \"非常停止ｲﾝﾀｰﾛｯｸ\"\n",
    "if regex_pattern_check(emergency_stop_comment, [rung_name_str]):\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b95e3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'emergency_stop': {\n",
    "        'rung_no': 2,\n",
    "        'next_rung': 8,\n",
    "        'range': '2-8',\n",
    "        'outcoil': [\n",
    "            'AL100[1]',\n",
    "            'AL100[2]',\n",
    "            'AL100[3]',\n",
    "            'AL100[4]',\n",
    "            'AL100[5]',\n",
    "            'AL100[11]'\n",
    "        ]\n",
    "    },\n",
    "    'emergency_stop_with_independence_device': {\n",
    "        'rung_no': -1,\n",
    "        'next_rung': -1,\n",
    "        'range': '',\n",
    "        'outcoil': []\n",
    "    },\n",
    "    'automatic_stop': {\n",
    "        'rung_no': -1,\n",
    "        'next_rung': -1,\n",
    "        'range': '',\n",
    "        'outcoil': []\n",
    "    },\n",
    "    'cycle_stop': {\n",
    "        'rung_no': 8,\n",
    "        'next_rung': 9,\n",
    "        'range': '8-9',\n",
    "        'outcoil': ['AL100[21]']\n",
    "    },\n",
    "    'fault_stop': {\n",
    "        'rung_no': 9,\n",
    "        'next_rung': 10,\n",
    "        'range': '9-10',\n",
    "        'outcoil': ['AL100[41]']\n",
    "    },\n",
    "    'warning': {\n",
    "        'rung_no': 10,\n",
    "        'next_rung': 81,\n",
    "        'range': '10-81',\n",
    "        'outcoil': [\n",
    "            'AL100[46]',\n",
    "            'LB200',\n",
    "            'LB210[0]',\n",
    "            'LB210[1]',\n",
    "            'LB210[2]',\n",
    "            'LB210[3]',\n",
    "            'LB210[9]',\n",
    "            'LB210[10]',\n",
    "            'LB210[11]',\n",
    "            'LB210[12]',\n",
    "            'LB210[13]',\n",
    "            'LB210[14]',\n",
    "            'LB210[18]',\n",
    "            'LB210[19]',\n",
    "            'LB211[0]',\n",
    "            'LB211[1]',\n",
    "            'LB211[9]',\n",
    "            'LB211[10]',\n",
    "            'LB211[11]',\n",
    "            'LB211[12]',\n",
    "            'LB211[13]',\n",
    "            'LB211[14]',\n",
    "            'LB211[18]',\n",
    "            'LB211[19]',\n",
    "            'LB212[0]',\n",
    "            'LB212[1]',\n",
    "            'LB212[2]',\n",
    "            'LB212[3]',\n",
    "            'LB212[4]',\n",
    "            'LB212[9]',\n",
    "            'LB212[10]',\n",
    "            'LB212[11]',\n",
    "            'LB212[12]',\n",
    "            'LB212[13]',\n",
    "            'LB212[14]',\n",
    "            'LB212[18]',\n",
    "            'LB212[19]',\n",
    "            'LB213[0]',\n",
    "            'LB213[1]',\n",
    "            'LB213[9]',\n",
    "            'LB213[10]',\n",
    "            'LB213[11]',\n",
    "            'LB213[12]',\n",
    "            'LB213[13]',\n",
    "            'LB213[14]',\n",
    "            'LB213[18]',\n",
    "            'LB213[19]',\n",
    "            'LB214[0]',\n",
    "            'LB214[1]',\n",
    "            'LB214[2]',\n",
    "            'LB214[3]',\n",
    "            'LB214[9]',\n",
    "            'LB214[10]',\n",
    "            'LB214[11]',\n",
    "            'LB214[12]',\n",
    "            'LB214[13]',\n",
    "            'LB214[14]',\n",
    "            'LB214[18]',\n",
    "            'LB214[19]',\n",
    "            'LB215[10]',\n",
    "            'LB215[11]',\n",
    "            'LB215[12]',\n",
    "            'LB215[13]',\n",
    "            'LB215[19]',\n",
    "            'LB220[0]',\n",
    "            'LB220[1]',\n",
    "            'LB220[2]',\n",
    "            'LB220[3]',\n",
    "            'BZ',\n",
    "            'LB220[9]',\n",
    "            'ST_Y',\n",
    "            'ST_R'\n",
    "        ]\n",
    "    },\n",
    "    'warning_with_independence_device': {\n",
    "        'rung_no': -1,\n",
    "        'next_rung': -1,\n",
    "        'range': '',\n",
    "        'outcoil': []\n",
    "    },\n",
    "    'operation_fault': {\n",
    "        'rung_no': -1,\n",
    "        'next_rung': -1,\n",
    "        'range': '',\n",
    "        'outcoil': []\n",
    "    }\n",
    "}\n",
    "{\n",
    "    'emergency_stop_total': {\n",
    "        'rung_no': 12,\n",
    "        'next_rung': 24,\n",
    "        'range': '12-24',\n",
    "        'outcoil': [\n",
    "            'LB210[0]',\n",
    "            'LB210[1]',\n",
    "            'LB210[2]',\n",
    "            'LB210[3]',\n",
    "            'LB210[9]',\n",
    "            'LB210[10]',\n",
    "            'LB210[11]',\n",
    "            'LB210[12]',\n",
    "            'LB210[13]',\n",
    "            'LB210[14]',\n",
    "            'LB210[18]',\n",
    "            'LB210[19]'\n",
    "        ]\n",
    "    },\n",
    "    'automatic_stop_total': {\n",
    "        'rung_no': 24,\n",
    "        'next_rung': 34,\n",
    "        'range': '24-34',\n",
    "        'outcoil': [\n",
    "            'LB211[0]',\n",
    "            'LB211[1]',\n",
    "            'LB211[9]',\n",
    "            'LB211[10]',\n",
    "            'LB211[11]',\n",
    "            'LB211[12]',\n",
    "            'LB211[13]',\n",
    "            'LB211[14]',\n",
    "            'LB211[18]',\n",
    "            'LB211[19]'\n",
    "        ]\n",
    "    },\n",
    "    'cycle_stop_total': {\n",
    "        'rung_no': 34,\n",
    "        'next_rung': 47,\n",
    "        'range': '34-47',\n",
    "        'outcoil': [\n",
    "            'LB212[0]',\n",
    "            'LB212[1]',\n",
    "            'LB212[2]',\n",
    "            'LB212[3]',\n",
    "            'LB212[4]',\n",
    "            'LB212[9]',\n",
    "            'LB212[10]',\n",
    "            'LB212[11]',\n",
    "            'LB212[12]',\n",
    "            'LB212[13]',\n",
    "            'LB212[14]',\n",
    "            'LB212[18]',\n",
    "            'LB212[19]'\n",
    "        ]\n",
    "    },\n",
    "    'fault_stop_total': {\n",
    "        'rung_no': 47,\n",
    "        'next_rung': 57,\n",
    "        'range': '47-57',\n",
    "        'outcoil': [\n",
    "            'LB213[0]',\n",
    "            'LB213[1]',\n",
    "            'LB213[9]',\n",
    "            'LB213[10]',\n",
    "            'LB213[11]',\n",
    "            'LB213[12]',\n",
    "            'LB213[13]',\n",
    "            'LB213[14]',\n",
    "            'LB213[18]',\n",
    "            'LB213[19]'\n",
    "        ]\n",
    "    },\n",
    "    'warning_total': {\n",
    "        'rung_no': 57,\n",
    "        'next_rung': 81,\n",
    "        'range': '57-81',\n",
    "        'outcoil': [\n",
    "            'LB214[0]',\n",
    "            'LB214[1]',\n",
    "            'LB214[2]',\n",
    "            'LB214[3]',\n",
    "            'LB214[9]',\n",
    "            'LB214[10]',\n",
    "            'LB214[11]',\n",
    "            'LB214[12]',\n",
    "            'LB214[13]',\n",
    "            'LB214[14]',\n",
    "            'LB214[18]',\n",
    "            'LB214[19]',\n",
    "            'LB215[10]',\n",
    "            'LB215[11]',\n",
    "            'LB215[12]',\n",
    "            'LB215[13]',\n",
    "            'LB215[19]',\n",
    "            'LB220[0]',\n",
    "            'LB220[1]',\n",
    "            'LB220[2]',\n",
    "            'LB220[3]',\n",
    "            'BZ',\n",
    "            'LB220[9]',\n",
    "            'ST_Y',\n",
    "            'ST_R'\n",
    "        ]\n",
    "    },\n",
    "    'final_summary': {\n",
    "        'rung_no': -1,\n",
    "        'next_rung': -1,\n",
    "        'range': '',\n",
    "        'outcoil': ['ST_G']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import polars as pl\n",
    "import re, json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import copy\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, deque\n",
    "import ast\n",
    "import icecream as ic\n",
    "    \n",
    "def get_block_connections(ladder_df:pd.DataFrame)->List[Dict]:\n",
    "    \n",
    "    in_regex=r\"in_list\"\n",
    "    out_regex=r\"out_list\"\n",
    "    inout_regex=r\"inoutVar\"\n",
    "    in_var_regex=r\"inVar\"\n",
    "    out_var_regex=r\"outVar\"\n",
    "    \n",
    "    object_list=list(ladder_df['OBJECT'])\n",
    "    object_type_list=list(ladder_df['OBJECT_TYPE_LIST'])\n",
    "    attributes_list=list(ladder_df['ATTRIBUTES'])\n",
    "    \n",
    "    rest_object_list=list(ladder_df['OBJECT'])\n",
    "    rest_object_type_list=list(ladder_df['OBJECT_TYPE_LIST'])\n",
    "    rest_attributes_list=list(ladder_df['ATTRIBUTES'])\n",
    "    output_list=[]\n",
    "    \n",
    "    for block_count, (block_type, block_attr) in enumerate(zip(object_type_list, attributes_list)):\n",
    "        \n",
    "        block_attr=eval(block_attr)\n",
    "        \n",
    "        if (block_type=='Block') and (len(block_attr)!=0):\n",
    "            \n",
    "            block_keys=block_attr.keys()\n",
    "            block_values=block_attr.values()\n",
    "            \n",
    "            \n",
    "            out_block_dict={}\n",
    "            out_block_dict[block_attr['typeName']]=[]\n",
    "                       \n",
    "            \n",
    "            for block_key,block_value in zip(block_keys, block_values):\n",
    "                \n",
    "                for rest_block_count, (rest_block_type, rest_block_attr) in enumerate(zip(rest_object_type_list, rest_attributes_list)):\n",
    "                    \n",
    "                    if rest_block_count!=block_count:\n",
    "                    \n",
    "                        rest_block_attr=eval(rest_block_attr)\n",
    "                        \n",
    "                        if len(rest_block_attr)!=0:\n",
    "                            \n",
    "                            rest_block_attr_keys=rest_block_attr.keys()\n",
    "                            rest_block_attr_values=rest_block_attr.values()\n",
    "                            \n",
    "                            for rest_block_key, rest_block_value in zip(rest_block_attr_keys, rest_block_attr_values):\n",
    "                                \n",
    "                                temp_block_dict={}\n",
    "                                temp_block_conn_list=[]\n",
    "                                \n",
    "                                \n",
    "                                if (type(block_value) is list) and (type(rest_block_value) is list):\n",
    "                                    if set(block_value) & set(rest_block_value):\n",
    "                                        \n",
    "                                        if re.search(in_regex, block_key) and re.search(out_regex, rest_block_key):\n",
    "                                            \n",
    "                                            block_value_orig=block_key.split(\"_\")[0]\n",
    "                                            block_value_orig_exists=temp_block_dict.get(block_value_orig, 'NONE')\n",
    "                                                                                        \n",
    "                                            if rest_block_type=='DataSource':\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=[rest_block_attr['identifier']]\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append(rest_block_attr['identifier'])\n",
    "                                                    \n",
    "                                                    \n",
    "\n",
    "                                            if (rest_block_type=='Contact') or (rest_block_type=='Coil'):\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=[rest_block_attr['operand']]\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append(rest_block_attr['operand'])\n",
    "                                                    \n",
    "                                                    \n",
    "\n",
    "                                            if (rest_block_type=='LeftPowerRail'):\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=['LeftPowerRail']\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append('LeftPowerRail')\n",
    "                       \n",
    "                                            \n",
    "                                                                                   \n",
    "                                            \n",
    "                                            out_block_dict[block_attr['typeName']].append(temp_block_dict)\n",
    "                                            \n",
    "                                            \n",
    "                                            \n",
    "                                            \n",
    "                                        if re.search(out_regex, block_key) and re.search(in_regex, rest_block_key):\n",
    "                                            \n",
    "                                            block_value_orig=block_key.split(\"_\")[0]\n",
    "                                            block_value_orig_exists=temp_block_dict.get(block_value_orig, 'NONE')\n",
    "                                            \n",
    "\n",
    "                                            if rest_block_type=='DataSink':\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=[rest_block_attr['identifier']]\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append(rest_block_attr['identifier'])\n",
    "                                                    \n",
    "                                                    \n",
    "\n",
    "                                            if (rest_block_type=='Contact') or (rest_block_type=='Coil'):\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=[rest_block_attr['operand']]\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append(rest_block_attr['operand'])\n",
    "                                                    \n",
    "                                                    \n",
    "\n",
    "                                            if (rest_block_type=='RightPowerRail'):\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=['RightPowerRail']\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append('RightPowerRail')\n",
    "                                     \n",
    "                                            \n",
    "                                                                                   \n",
    "                                            \n",
    "                                            out_block_dict[block_attr['typeName']].append(temp_block_dict)\n",
    "                                            \n",
    "                                            \n",
    "            output_list.append(out_block_dict)\n",
    "    return(output_list)  \n",
    "\n",
    "\n",
    "############ Get series connection pairs #################\n",
    "\n",
    "def build_chains(data):\n",
    "    # Step 1: Build a map from outputs to list of dicts that consume them\n",
    "    index_map = {i: d for i, d in enumerate(data)}\n",
    "    chains = []\n",
    "\n",
    "    # Step 2: For each dict, attempt to build a chain forward\n",
    "    def dfs(path, visited):\n",
    "        last = path[-1]\n",
    "        extended = False\n",
    "\n",
    "        for i, candidate in index_map.items():\n",
    "            if i in visited:\n",
    "                continue\n",
    "            # Check for overlap between last out_list and candidate in_list\n",
    "            \n",
    "            out_list_present=last.get('out_list', [])\n",
    "            in_list_present=candidate.get('in_list', [])\n",
    "                    \n",
    "            \n",
    "            if set(out_list_present) & set(in_list_present):\n",
    "                dfs(path + [candidate], visited | {i})\n",
    "                extended = True\n",
    "\n",
    "        if not extended:\n",
    "            chains.append(path)\n",
    "\n",
    "    # Step 3: Start chains from each dictionary\n",
    "    for i in range(len(data)):\n",
    "        dfs([data[i]], {i})\n",
    "\n",
    "    return chains\n",
    "\n",
    "\n",
    "#############################################\n",
    "def create_super_sets(sub_list:List)->List:\n",
    "    \n",
    "    #This code block\n",
    "    sets = [set(lst) for lst in sub_list]\n",
    "\n",
    "    \n",
    "    visited = set()\n",
    "    groups = []\n",
    "\n",
    "    for i, s in enumerate(sets):\n",
    "        if not s or i in visited:\n",
    "            continue\n",
    "        group = set(s)\n",
    "        queue = [i]\n",
    "        visited.add(i)\n",
    "\n",
    "        while queue:\n",
    "            current = queue.pop()\n",
    "            for j, t in enumerate(sets):\n",
    "                if j not in visited and t and group & t:  # at least one common element\n",
    "                    group |= t\n",
    "                    visited.add(j)\n",
    "                    queue.append(j)\n",
    "\n",
    "        groups.append(sorted(group))\n",
    "\n",
    "    # Include completely empty lists if they were in the original\n",
    "    if any(not lst for lst in sub_list):\n",
    "        groups.append([])\n",
    "\n",
    "    \n",
    "    return groups\n",
    "\n",
    "\n",
    "\n",
    "################### replace with sub_lists with super lists###############3\n",
    "\n",
    "def replace_sub_list_with_super_list(single:List, multi:List[List])->List:\n",
    "    \n",
    "    replacement = None\n",
    "\n",
    "    # Find the first sublist that intersects with single\n",
    "    for group in multi:\n",
    "        if set(single) & set(group):\n",
    "            replacement = group\n",
    "            break   \n",
    "                \n",
    "        \n",
    "    if replacement:\n",
    "        single = replacement   \n",
    "        \n",
    "        \n",
    "    return single\n",
    "\n",
    "      \n",
    "############### get Unique Dicts#############3\n",
    "\n",
    "def dict_to_tuple(d):\n",
    "    return tuple(sorted((k, tuple(v) if isinstance(v, list) else v) for k, v in d.items()))\n",
    "\n",
    "def get_unique_dicts(in_list:List)->List:\n",
    "    \n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for d in in_list:\n",
    "        key = dict_to_tuple(d)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(d) \n",
    "                       \n",
    "    return unique\n",
    "               \n",
    "\n",
    "\n",
    "\n",
    "######################## get the Parallel contacts##########################\n",
    "def get_parallel_contacts(ladder_df:pd.DataFrame)->List:\n",
    "    \n",
    "    attribute_list=list(ladder_df['ATTRIBUTES'])\n",
    "    attribute_list=[eval(ele) for ele in attribute_list]\n",
    "    object_type_list=list(ladder_df['OBJECT_TYPE_LIST'])\n",
    "    new_attribute_list=[]\n",
    "    \n",
    "    \n",
    "    #AAdd the object type to the dictionaries\n",
    "    for attr_ , object_type in zip(attribute_list, object_type_list):\n",
    "        attr_['object_type']=object_type\n",
    "        new_attribute_list.append(attr_)\n",
    "      \n",
    "    \n",
    "    chains=build_chains(new_attribute_list)\n",
    "    \n",
    "    #Create a superlist merging all the inlist and outlist, and subsume the sublist in to superlist\n",
    "    super_list=[]\n",
    "    \n",
    "    for attr_ in attribute_list:\n",
    "        \n",
    "        attr_in_list=attr_.get('in_list', [])\n",
    "        attr_out_list=attr_.get('out_list', [])\n",
    "        \n",
    "        super_list.append(attr_in_list)\n",
    "        super_list.append(attr_out_list)\n",
    " \n",
    "    merged_super_list=create_super_sets(super_list)\n",
    "    \n",
    "    chain_with_super_nodes=[]\n",
    "    \n",
    "    \n",
    "\n",
    "    #Modify the merged super list with super nodes\n",
    "    for ele_dict in chains:\n",
    "        subchain_with_super_nodes=[]\n",
    "        for ele in ele_dict:\n",
    "            \n",
    "            in_list_super_set=replace_sub_list_with_super_list( ele.get('in_list', []) , merged_super_list)\n",
    "            out_list_super_set=replace_sub_list_with_super_list(  ele.get('out_list', []),   merged_super_list)\n",
    "            \n",
    "            ele['in_list']=in_list_super_set\n",
    "            ele['out_list']=out_list_super_set\n",
    "            \n",
    "            subchain_with_super_nodes.append(ele)\n",
    "            \n",
    "        chain_with_super_nodes.append(subchain_with_super_nodes)\n",
    "        \n",
    "        \n",
    "    #Create attributes with super lists\n",
    "    attributes_with_super_nodes=[]\n",
    "    \n",
    "    for item in new_attribute_list:\n",
    "        \n",
    "         \n",
    "         in_list_super_set=replace_sub_list_with_super_list(item['in_list'], merged_super_list)\n",
    "         out_list_super_set=replace_sub_list_with_super_list(item['out_list'], merged_super_list)\n",
    "         \n",
    "         item['in_list']=in_list_super_set\n",
    "         item['out_list']=out_list_super_set\n",
    "         \n",
    "         attributes_with_super_nodes.append(item)\n",
    "         \n",
    "       \n",
    "    #Extract the parallel pairs\n",
    "    parallel_pairs={}\n",
    "    \n",
    "    for indiv_count,indiv in enumerate(attributes_with_super_nodes):\n",
    "        \n",
    "        if (indiv['object_type']=='Contact'):\n",
    "            \n",
    "            \n",
    "        \n",
    "            for chain_count, chain in enumerate(chain_with_super_nodes):\n",
    "                \n",
    "                start_point=0\n",
    "                contact_stack=[]\n",
    "                \n",
    "                \n",
    "                \n",
    "                for sub_dict in chain:\n",
    "                   \n",
    "                    \n",
    "                    if (sub_dict['object_type']=='Contact'):\n",
    "                        \n",
    "                        if indiv!=sub_dict:\n",
    "                                                                  \n",
    "                            \n",
    "                            if (indiv['in_list']==sub_dict['in_list']):\n",
    "                                start_point=1\n",
    "                                contact_stack.append(sub_dict)\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                            if (indiv['out_list']==sub_dict['out_list']) and (start_point==1):\n",
    "                                \n",
    "                                \n",
    "                                pair_dict={}\n",
    "                                contact_stack.append(sub_dict)\n",
    "                                \n",
    "                                \n",
    "                                contact_stack=get_unique_dicts(contact_stack)\n",
    "                                pair_dict['contact_chain']=contact_stack\n",
    "                                \n",
    "                                                               \n",
    "                                # pair_dict['start_contact']=contact_stack\n",
    "                                # pair_dict['stop_contact']=sub_dict\n",
    "                                pair_dict['ref_contact']=indiv\n",
    "                                \n",
    "                                parallel_pairs[f\"{indiv_count}_{chain_count}\"]=pair_dict\n",
    "                                                        \n",
    "                                break\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                            \n",
    "    #Remove the duplicates\n",
    "    seen = set()\n",
    "    unique_pairs = {}\n",
    "\n",
    "    for key, value in parallel_pairs.items():\n",
    "        serialized = json.dumps(value, sort_keys=True)\n",
    "        if serialized not in seen:\n",
    "            seen.add(serialized)\n",
    "            unique_pairs[key] = value                    \n",
    "            \n",
    "\n",
    " \n",
    "    \n",
    "    return unique_pairs\n",
    "    \n",
    "########################################################3\n",
    "\n",
    "def get_in_parallel_A_contacts(contact_chain_list:List, contact:str)-> List:\n",
    "    \n",
    "    in_parallel_list=[]\n",
    "    for contact_chain in contact_chain_list:\n",
    "        \n",
    "        ref_contact=contact_chain['ref_contact']['operand']\n",
    "        chain= contact_chain['contact_chain']\n",
    "        \n",
    "        if ref_contact==contact:\n",
    "            for contact_operand in chain:\n",
    "                \n",
    "                contact_operand_negated_status=contact_operand.get(\"negated\", 'NONE')\n",
    "                \n",
    "                if all([contact_operand_negated_status!='NONE', contact_operand_negated_status=='false']):\n",
    "                    \n",
    "                \n",
    "                    in_parallel_list.append(contact_operand['operand'])\n",
    "                \n",
    "            \n",
    "        \n",
    "    return in_parallel_list \n",
    "\n",
    "\n",
    "\n",
    "##########################Rule 51 for getting pareelel contact detail ######################3\n",
    "# Final result\n",
    "def get_format_parellel_contact_detail(data):\n",
    "    grouped_data = defaultdict(list)\n",
    "\n",
    "    for entry in data.values():\n",
    "        ref_operand = entry['ref_contact']['operand']\n",
    "        contact_operands = [contact['operand'] for contact in entry['contact_chain']]\n",
    "\n",
    "        # Avoid adding duplicate operand chains for same ref_contact\n",
    "        if contact_operands not in grouped_data[ref_operand]:\n",
    "            grouped_data[ref_operand].append(contact_operands)\n",
    "\n",
    "    # Convert defaultdict to regular dict for output\n",
    "    grouped_data = dict(grouped_data)\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'=': [{'In1': ['LD200']}, {'In2': ['DWORD#16#000B0000']}, {'': ['AL000[31]']}]}]\n"
     ]
    }
   ],
   "source": [
    "input_program_file = r\"C:\\Users\\aniln\\OneDrive - OPTIMIZED SOLUTIONS LTD\\DENSO\\GithubCode\\rules_personal\\FASTAPI_CODE\\FASTAPI_POC_DENSO\\input_files\\Coding Checker_Rule44_56NG_250619\\Coding Checker_Rule44_56NG_250619_programwise.csv\"\n",
    "program_df = pd.read_csv(input_program_file)\n",
    "current_program_df = program_df[program_df['PROGRAM'] == \"P000_InitialSetting\"]\n",
    "fault_section_df = current_program_df[current_program_df['BODY'].str.lower() == 'fault']\n",
    "current_coil_df = fault_section_df[fault_section_df['RUNG']==21]\n",
    "all_self_holding = get(current_coil_df)\n",
    "print(all_self_holding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf9764a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task name</th>\n",
       "      <th>Process type</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Process No</th>\n",
       "      <th>Machine Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P000_IntialSetting</td>\n",
       "      <td>Othrer</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P002_ServoAdjustment</td>\n",
       "      <td>Othrer</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P003_Servo_Out</td>\n",
       "      <td>Othrer</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P009_Operation_Assist</td>\n",
       "      <td>Othrer</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P100_Main1</td>\n",
       "      <td>Othrer</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P101_HMI</td>\n",
       "      <td>Othrer</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P111_XXXPRS_Function1</td>\n",
       "      <td>Transport</td>\n",
       "      <td>P&amp;P</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Task name Process type  Unit Process No  Machine Number\n",
       "0     P000_IntialSetting       Othrer  NONE       NONE             6.0\n",
       "1   P002_ServoAdjustment       Othrer  NONE       NONE             NaN\n",
       "2         P003_Servo_Out       Othrer  NONE       NONE             NaN\n",
       "3  P009_Operation_Assist       Othrer  NONE       NONE             NaN\n",
       "4             P100_Main1       Othrer  NONE       NONE             NaN\n",
       "5               P101_HMI       Othrer  NONE       NONE             NaN\n",
       "6  P111_XXXPRS_Function1    Transport   P&P          1             NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\aniln\\OneDrive - OPTIMIZED SOLUTIONS LTD\\DENSO\\Denso\\Ladder_pdf_xml_rule\\input_image_files\\Input_Rule4.1-4.3_250808.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2055b066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['Process No'].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e719db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import polars as pl\n",
    "import re, json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import copy\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, deque\n",
    "import ast\n",
    "import icecream as ic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d628a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import polars as pl\n",
    "import re, json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import copy\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, deque\n",
    "import ast\n",
    "import icecream as ic\n",
    "    \n",
    "def get_block_connections(ladder_df:pd.DataFrame)->List[Dict]:\n",
    "    \n",
    "    in_regex=r\"in_list\"\n",
    "    out_regex=r\"out_list\"\n",
    "    inout_regex=r\"inoutVar\"\n",
    "    in_var_regex=r\"inVar\"\n",
    "    out_var_regex=r\"outVar\"\n",
    "    \n",
    "    object_list=list(ladder_df['OBJECT'])\n",
    "    object_type_list=list(ladder_df['OBJECT_TYPE_LIST'])\n",
    "    attributes_list=list(ladder_df['ATTRIBUTES'])\n",
    "    \n",
    "    rest_object_list=list(ladder_df['OBJECT'])\n",
    "    rest_object_type_list=list(ladder_df['OBJECT_TYPE_LIST'])\n",
    "    rest_attributes_list=list(ladder_df['ATTRIBUTES'])\n",
    "    output_list=[]\n",
    "    \n",
    "    for block_count, (block_type, block_attr) in enumerate(zip(object_type_list, attributes_list)):\n",
    "        \n",
    "        block_attr=eval(block_attr)\n",
    "        \n",
    "        if (block_type=='Block') and (len(block_attr)!=0):\n",
    "            \n",
    "            block_keys=block_attr.keys()\n",
    "            block_values=block_attr.values()\n",
    "            \n",
    "            \n",
    "            out_block_dict={}\n",
    "            out_block_dict[block_attr['typeName']]=[]\n",
    "                       \n",
    "            \n",
    "            for block_key,block_value in zip(block_keys, block_values):\n",
    "                \n",
    "                for rest_block_count, (rest_block_type, rest_block_attr) in enumerate(zip(rest_object_type_list, rest_attributes_list)):\n",
    "                    \n",
    "                    if rest_block_count!=block_count:\n",
    "                    \n",
    "                        rest_block_attr=eval(rest_block_attr)\n",
    "                        \n",
    "                        if len(rest_block_attr)!=0:\n",
    "                            \n",
    "                            rest_block_attr_keys=rest_block_attr.keys()\n",
    "                            rest_block_attr_values=rest_block_attr.values()\n",
    "                            \n",
    "                            for rest_block_key, rest_block_value in zip(rest_block_attr_keys, rest_block_attr_values):\n",
    "                                \n",
    "                                temp_block_dict={}\n",
    "                                temp_block_conn_list=[]\n",
    "                                \n",
    "                                \n",
    "                                if (type(block_value) is list) and (type(rest_block_value) is list):\n",
    "                                    if set(block_value) & set(rest_block_value):\n",
    "                                        \n",
    "                                        if re.search(in_regex, block_key) and re.search(out_regex, rest_block_key):\n",
    "                                            \n",
    "                                            block_value_orig=block_key.split(\"_\")[0]\n",
    "                                            block_value_orig_exists=temp_block_dict.get(block_value_orig, 'NONE')\n",
    "                                                                                        \n",
    "                                            if rest_block_type=='DataSource':\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=[rest_block_attr['identifier']]\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append(rest_block_attr['identifier'])\n",
    "                                                    \n",
    "                                                    \n",
    "\n",
    "                                            if (rest_block_type=='Contact') or (rest_block_type=='Coil'):\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=[rest_block_attr['operand']]\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append(rest_block_attr['operand'])\n",
    "                                                    \n",
    "                                                    \n",
    "\n",
    "                                            if (rest_block_type=='LeftPowerRail'):\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=['LeftPowerRail']\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append('LeftPowerRail')\n",
    "                       \n",
    "                                            \n",
    "                                                                                   \n",
    "                                            \n",
    "                                            out_block_dict[block_attr['typeName']].append(temp_block_dict)\n",
    "                                            \n",
    "                                            \n",
    "                                            \n",
    "                                            \n",
    "                                        if re.search(out_regex, block_key) and re.search(in_regex, rest_block_key):\n",
    "                                            \n",
    "                                            block_value_orig=block_key.split(\"_\")[0]\n",
    "                                            block_value_orig_exists=temp_block_dict.get(block_value_orig, 'NONE')\n",
    "                                            \n",
    "\n",
    "                                            if rest_block_type=='DataSink':\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=[rest_block_attr['identifier']]\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append(rest_block_attr['identifier'])\n",
    "                                                    \n",
    "                                                    \n",
    "\n",
    "                                            if (rest_block_type=='Contact') or (rest_block_type=='Coil'):\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=[rest_block_attr['operand']]\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append(rest_block_attr['operand'])\n",
    "                                                    \n",
    "                                                    \n",
    "\n",
    "                                            if (rest_block_type=='RightPowerRail'):\n",
    "                                                if block_value_orig_exists=='NONE':\n",
    "                                                    temp_block_dict[block_value_orig]=['RightPowerRail']\n",
    "                                                    \n",
    "                                                else:\n",
    "                                                    temp_block_dict[block_value_orig].append('RightPowerRail')\n",
    "                                     \n",
    "                                            \n",
    "                                                                                   \n",
    "                                            \n",
    "                                            out_block_dict[block_attr['typeName']].append(temp_block_dict)\n",
    "                                            \n",
    "                                            \n",
    "            output_list.append(out_block_dict)\n",
    "    return(output_list)  \n",
    "\n",
    "\n",
    "############ Get series connection pairs #################\n",
    "\n",
    "def build_chains(data):\n",
    "    # Step 1: Build a map from outputs to list of dicts that consume them\n",
    "    index_map = {i: d for i, d in enumerate(data)}\n",
    "    chains = []\n",
    "\n",
    "    # Step 2: For each dict, attempt to build a chain forward\n",
    "    def dfs(path, visited):\n",
    "        last = path[-1]\n",
    "        extended = False\n",
    "\n",
    "        for i, candidate in index_map.items():\n",
    "            if i in visited:\n",
    "                continue\n",
    "            # Check for overlap between last out_list and candidate in_list\n",
    "            \n",
    "            out_list_present=last.get('out_list', [])\n",
    "            in_list_present=candidate.get('in_list', [])\n",
    "                    \n",
    "            \n",
    "            if set(out_list_present) & set(in_list_present):\n",
    "                dfs(path + [candidate], visited | {i})\n",
    "                extended = True\n",
    "\n",
    "        if not extended:\n",
    "            chains.append(path)\n",
    "\n",
    "    # Step 3: Start chains from each dictionary\n",
    "    for i in range(len(data)):\n",
    "        dfs([data[i]], {i})\n",
    "\n",
    "    return chains\n",
    "\n",
    "\n",
    "#############################################\n",
    "def create_super_sets(sub_list:List)->List:\n",
    "    \n",
    "    #This code block\n",
    "    sets = [set(lst) for lst in sub_list]\n",
    "\n",
    "    \n",
    "    visited = set()\n",
    "    groups = []\n",
    "\n",
    "    for i, s in enumerate(sets):\n",
    "        if not s or i in visited:\n",
    "            continue\n",
    "        group = set(s)\n",
    "        queue = [i]\n",
    "        visited.add(i)\n",
    "\n",
    "        while queue:\n",
    "            current = queue.pop()\n",
    "            for j, t in enumerate(sets):\n",
    "                if j not in visited and t and group & t:  # at least one common element\n",
    "                    group |= t\n",
    "                    visited.add(j)\n",
    "                    queue.append(j)\n",
    "\n",
    "        groups.append(sorted(group))\n",
    "\n",
    "    # Include completely empty lists if they were in the original\n",
    "    if any(not lst for lst in sub_list):\n",
    "        groups.append([])\n",
    "\n",
    "    \n",
    "    return groups\n",
    "\n",
    "\n",
    "\n",
    "################### replace with sub_lists with super lists###############3\n",
    "\n",
    "def replace_sub_list_with_super_list(single:List, multi:List[List])->List:\n",
    "    \n",
    "    replacement = None\n",
    "\n",
    "    # Find the first sublist that intersects with single\n",
    "    for group in multi:\n",
    "        if set(single) & set(group):\n",
    "            replacement = group\n",
    "            break   \n",
    "                \n",
    "        \n",
    "    if replacement:\n",
    "        single = replacement   \n",
    "        \n",
    "        \n",
    "    return single\n",
    "\n",
    "      \n",
    "############### get Unique Dicts#############3\n",
    "\n",
    "def dict_to_tuple(d):\n",
    "    return tuple(sorted((k, tuple(v) if isinstance(v, list) else v) for k, v in d.items()))\n",
    "\n",
    "def get_unique_dicts(in_list:List)->List:\n",
    "    \n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for d in in_list:\n",
    "        key = dict_to_tuple(d)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(d) \n",
    "                       \n",
    "    return unique\n",
    "               \n",
    "\n",
    "\n",
    "\n",
    "######################## get the Parallel contacts##########################\n",
    "def get_parallel_contacts(ladder_df:pd.DataFrame)->List:\n",
    "    \n",
    "    attribute_list=list(ladder_df['ATTRIBUTES'])\n",
    "    attribute_list=[eval(ele) for ele in attribute_list]\n",
    "    object_type_list=list(ladder_df['OBJECT_TYPE_LIST'])\n",
    "    new_attribute_list=[]\n",
    "    \n",
    "    # print(\"object_type_list\",object_type_list)\n",
    "    #AAdd the object type to the dictionaries\n",
    "    for attr_ , object_type in zip(attribute_list, object_type_list):\n",
    "        attr_['object_type']=object_type\n",
    "        new_attribute_list.append(attr_)\n",
    "      \n",
    "    \n",
    "    chains=build_chains(new_attribute_list)\n",
    "    \n",
    "    #Create a superlist merging all the inlist and outlist, and subsume the sublist in to superlist\n",
    "    super_list=[]\n",
    "    \n",
    "    for attr_ in attribute_list:\n",
    "        \n",
    "        attr_in_list=attr_.get('in_list', [])\n",
    "        attr_out_list=attr_.get('out_list', [])\n",
    "        \n",
    "        super_list.append(attr_in_list)\n",
    "        super_list.append(attr_out_list)\n",
    " \n",
    "    merged_super_list=create_super_sets(super_list)\n",
    "    \n",
    "    chain_with_super_nodes=[]\n",
    "    \n",
    "    \n",
    "\n",
    "    #Modify the merged super list with super nodes\n",
    "    for ele_dict in chains:\n",
    "        subchain_with_super_nodes=[]\n",
    "        for ele in ele_dict:\n",
    "            \n",
    "            in_list_super_set=replace_sub_list_with_super_list( ele.get('in_list', []) , merged_super_list)\n",
    "            out_list_super_set=replace_sub_list_with_super_list(  ele.get('out_list', []),   merged_super_list)\n",
    "            \n",
    "            ele['in_list']=in_list_super_set\n",
    "            ele['out_list']=out_list_super_set\n",
    "            \n",
    "            subchain_with_super_nodes.append(ele)\n",
    "            \n",
    "        chain_with_super_nodes.append(subchain_with_super_nodes)\n",
    "        \n",
    "        \n",
    "    #Create attributes with super lists\n",
    "    attributes_with_super_nodes=[]\n",
    "    \n",
    "    for item in new_attribute_list:\n",
    "        \n",
    "         \n",
    "         in_list_super_set=replace_sub_list_with_super_list(item['in_list'], merged_super_list)\n",
    "         out_list_super_set=replace_sub_list_with_super_list(item['out_list'], merged_super_list)\n",
    "         \n",
    "         item['in_list']=in_list_super_set\n",
    "         item['out_list']=out_list_super_set\n",
    "         \n",
    "         attributes_with_super_nodes.append(item)\n",
    "         \n",
    "       \n",
    "    #Extract the parallel pairs\n",
    "    parallel_pairs={}\n",
    "    \n",
    "    for indiv_count,indiv in enumerate(attributes_with_super_nodes):\n",
    "        \n",
    "        if (indiv['object_type']=='Contact'):\n",
    "            \n",
    "            \n",
    "        \n",
    "            for chain_count, chain in enumerate(chain_with_super_nodes):\n",
    "                \n",
    "                start_point=0\n",
    "                contact_stack=[]\n",
    "                \n",
    "                \n",
    "                \n",
    "                for sub_dict in chain:\n",
    "                   \n",
    "                    \n",
    "                    if (sub_dict['object_type']=='Contact'):\n",
    "                        \n",
    "                        if indiv!=sub_dict:\n",
    "                                                                  \n",
    "                            \n",
    "                            if (indiv['in_list']==sub_dict['in_list']):\n",
    "                                start_point=1\n",
    "                                contact_stack.append(sub_dict)\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                            if (indiv['out_list']==sub_dict['out_list']) and (start_point==1):\n",
    "                                \n",
    "                                \n",
    "                                pair_dict={}\n",
    "                                contact_stack.append(sub_dict)\n",
    "                                \n",
    "                                \n",
    "                                contact_stack=get_unique_dicts(contact_stack)\n",
    "                                pair_dict['contact_chain']=contact_stack\n",
    "                                \n",
    "                                                               \n",
    "                                # pair_dict['start_contact']=contact_stack\n",
    "                                # pair_dict['stop_contact']=sub_dict\n",
    "                                pair_dict['ref_contact']=indiv\n",
    "                                \n",
    "                                parallel_pairs[f\"{indiv_count}_{chain_count}\"]=pair_dict\n",
    "                                                        \n",
    "                                break\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                            \n",
    "    #Remove the duplicates\n",
    "    seen = set()\n",
    "    unique_pairs = {}\n",
    "\n",
    "    for key, value in parallel_pairs.items():\n",
    "        serialized = json.dumps(value, sort_keys=True)\n",
    "        if serialized not in seen:\n",
    "            seen.add(serialized)\n",
    "            unique_pairs[key] = value                    \n",
    "            \n",
    "\n",
    " \n",
    "    \n",
    "    return unique_pairs\n",
    "    \n",
    "########################################################3\n",
    "\n",
    "def get_in_parallel_A_contacts(contact_chain_list:List, contact:str)-> List:\n",
    "    \n",
    "    in_parallel_list=[]\n",
    "    for contact_chain in contact_chain_list:\n",
    "        \n",
    "        ref_contact=contact_chain['ref_contact']['operand']\n",
    "        chain= contact_chain['contact_chain']\n",
    "        \n",
    "        if ref_contact==contact:\n",
    "            for contact_operand in chain:\n",
    "                \n",
    "                contact_operand_negated_status=contact_operand.get(\"negated\", 'NONE')\n",
    "                \n",
    "                if all([contact_operand_negated_status!='NONE', contact_operand_negated_status=='false']):\n",
    "                    \n",
    "                \n",
    "                    in_parallel_list.append(contact_operand['operand'])\n",
    "                \n",
    "            \n",
    "        \n",
    "    return in_parallel_list \n",
    "\n",
    "\n",
    "\n",
    "##########################Rule 51 for getting pareelel contact detail ######################3\n",
    "# Final result\n",
    "def get_format_parellel_contact_detail(data):\n",
    "    grouped_data = defaultdict(list)\n",
    "\n",
    "    for entry in data.values():\n",
    "        ref_operand = entry['ref_contact']['operand']\n",
    "        contact_operands = [contact['operand'] for contact in entry['contact_chain']]\n",
    "\n",
    "        # Avoid adding duplicate operand chains for same ref_contact\n",
    "        if contact_operands not in grouped_data[ref_operand]:\n",
    "            grouped_data[ref_operand].append(contact_operands)\n",
    "\n",
    "    # Convert defaultdict to regular dict for output\n",
    "    grouped_data = dict(grouped_data)\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1f83943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'in_list': ['6'],\n",
      "   'negated': 'false',\n",
      "   'operand': 'T110_0101A1',\n",
      "   'out_list': ['7']},\n",
      "  {'in_list': ['7', '11'],\n",
      "   'negated': 'false',\n",
      "   'operand': 'GB112_PR.NoINTRF',\n",
      "   'out_list': ['12']}],\n",
      " [{'in_list': ['10'],\n",
      "   'negated': 'false',\n",
      "   'operand': 'T110_0101B1',\n",
      "   'out_list': ['11']},\n",
      "  {'in_list': ['7', '11'],\n",
      "   'negated': 'false',\n",
      "   'operand': 'GB112_PR.NoINTRF',\n",
      "   'out_list': ['12']}],\n",
      " [{'in_list': ['7', '11'],\n",
      "   'negated': 'false',\n",
      "   'operand': 'GB112_PR.NoINTRF',\n",
      "   'out_list': ['12']}],\n",
      " [{'in_list': ['3'],\n",
      "   'negated': 'false',\n",
      "   'operand': 'AL111[12]',\n",
      "   'out_list': ['16']}]]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "program = \"P111_sampleOK\"\n",
    "input_program_file = r\"C:\\Users\\aniln\\OneDrive - OPTIMIZED SOLUTIONS LTD\\DENSO\\GithubCode\\rules_personal\\FASTAPI_CODE\\FASTAPI_POC_DENSO\\input_files\\Coding Checker_Rule35_98_250731\\Coding Checker_Rule35_98_250731_programwise.csv\"\n",
    "program_df = pd.read_csv(input_program_file)\n",
    "curr_program_df = program_df[program_df[\"PROGRAM\"] == program].copy()\n",
    "curr_program_df = curr_program_df[curr_program_df['BODY'] == \"Fault\"]\n",
    "current_rung_df = curr_program_df[curr_program_df['RUNG'] == 5]\n",
    "# print(\"current_rung_df\",current_rung_df)\n",
    "block_connection_details = get_series_contacts(pl.from_pandas(current_rung_df))\n",
    "pprint.pprint(block_connection_details)\n",
    "\n",
    "# for block_data in block_connection_details:\n",
    "#     for block_name, params in block_data.items():\n",
    "#         if block_name.lower() == \"clear\":\n",
    "#             print(\"block_name\",block_name,\"params\",params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06c4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7cfc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_to_half_conversion = {\"ア\":\"ｱ\",\n",
    "\"イ\":\"ｲ\",\n",
    "\"ー\":\"ｰ\",\n",
    "\"ウ\":\"ｳ\",\n",
    "\"エ\":\"ｴ\",\n",
    "\"オ\":\"ｵ\",\n",
    "\"カ\":\"ｶ\",\n",
    "\"キ\":\"ｷ\",\n",
    "\"ク\":\"ｸ\",\n",
    "\"ケ\":\"ｹ\",\n",
    "\"コ\":\"ｺ\",\n",
    "\"サ\":\"ｻ\",\n",
    "\"シ\":\"ｼ\",\n",
    "\"ス\":\"ｽ\",\n",
    "\"セ\":\"ｾ\",\n",
    "\"ソ\":\"ｿ\",\n",
    "\"タ\":\"ﾀ\",\n",
    "\"チ\":\"ﾁ\",\n",
    "\"ツ\":\"ﾂ\",\n",
    "\"テ\":\"ﾃ\",\n",
    "\"ト\":\"ﾄ\",\n",
    "\"ナ\":\"ﾅ\",\n",
    "\"ニ\":\"ﾆ\",\n",
    "\"ヌ\":\"ﾇ\",\n",
    "\"ネ\":\"ﾈ\",\n",
    "\"ノ\":\"ﾉ\",\n",
    "\"ハ\":\"ﾊ\",\n",
    "\"ヒ\":\"ﾋ\",\n",
    "\"フ\":\"ﾌ\",\n",
    "\"ヘ\":\"ﾍ\",\n",
    "\"ホ\":\"ﾎ\",\n",
    "\"マ\":\"ﾏ\",\n",
    "\"ミ\":\"ﾐ\",\n",
    "\"ム\":\"ﾑ\",\n",
    "\"メ\":\"ﾒ\",\n",
    "\"モ\":\"ﾓ\",\n",
    "\"ヤ\":\"ﾔ\",\n",
    "\"ユ\":\"ﾕ\",\n",
    "\"ヨ\":\"ﾖ\",\n",
    "\"ラ\":\"ﾗ\",\n",
    "\"リ\":\"ﾘ\",\n",
    "\"ル\":\"ﾙ\",\n",
    "\"レ\":\"ﾚ\",\n",
    "\"ロ\":\"ﾛ\",\n",
    "\"ワ\":\"ﾜ\",\n",
    "\"ヲ\":\"ｦ\",\n",
    "\"ン\":\"ﾝ\",\n",
    "\"ガ\":\"ｶﾞ\",\n",
    "\"ギ\":\"ｷﾞ\",\n",
    "\"グ\":\"ｸﾞ\",\n",
    "\"ゲ\":\"ｹﾞ\",\n",
    "\"ゴ\":\"ｺﾞ\",\n",
    "\"ザ\":\"ｻﾞ\",\n",
    "\"ジ\":\"ｼﾞ\",\n",
    "\"ズ\":\"ｽﾞ\",\n",
    "\"ゼ\":\"ｾﾞ\",\n",
    "\"ゾ\":\"ｿﾞ\",\n",
    "\"ダ\":\"ﾀﾞ\",\n",
    "\"ヂ\":\"ﾁﾞ\",\n",
    "\"ヅ\":\"ﾂﾞ\",\n",
    "\"デ\":\"ﾃﾞ\",\n",
    "\"ド\":\"ﾄﾞ\",\n",
    "\"バ\":\"ﾊﾞ\",\n",
    "\"ビ\":\"ﾋﾞ\",\n",
    "\"ブ\":\"ﾌﾞ\",\n",
    "\"ベ\":\"ﾍﾞ\",\n",
    "\"ボ\":\"ﾎﾞ\",\n",
    "\"パ\":\"ﾊﾟ\",\n",
    "\"ピ\":\"ﾋﾟ\",\n",
    "\"プ\":\"ﾌﾟ\",\n",
    "\"ペ\":\"ﾍﾟ\",\n",
    "\"ポ\":\"ﾎﾟ\",\n",
    "\"ァ\":\"ｧ\",\n",
    "\"ィ\":\"ｨ\",\n",
    "\"ゥ\":\"ｩ\",\n",
    "\"ェ\":\"ｪ\",\n",
    "\"ォ\":\"ｫ\",\n",
    "\"ャ\":\"ｬ\",\n",
    "\"ュ\":\"ｭ\",\n",
    "\"ョ\":\"ｮ\",\n",
    "\"ッ\":\"ｯ\",\n",
    "\"ヮ\":\"ﾜ\",\n",
    "\"ヴ\":\"ｳﾞ\",\n",
    "\"ヵ\":\"ｶ\",\n",
    "\"ヶ\":\"ｹ\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regex_pattern_check(check_pattern:str, comment_list:list)->bool:\n",
    "    \n",
    "    ret_flag=0\n",
    "\n",
    "    if check_pattern and isinstance(check_pattern, str) and comment_list and isinstance(comment_list, list):\n",
    "        check_pattern = ''.join(full_to_half_conversion.get(char, char) for char in check_pattern)\n",
    "        for comment in comment_list:\n",
    "            if comment and isinstance(comment, str):\n",
    "                half_width_convert_comment = ''.join(full_to_half_conversion.get(char, char) for char in comment)\n",
    "                print(\"checkpattern\", check_pattern, \"half_width_convert_comment\",half_width_convert_comment)\n",
    "                if re.search(check_pattern, half_width_convert_comment):\n",
    "                    ret_flag=1\n",
    "                    break\n",
    "                \n",
    "        if ret_flag==0:\n",
    "            return False\n",
    "        else:\n",
    "            \n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44bd36de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpattern ﾜｰｸ half_width_convert_comment GlobalVars\n",
      "checkpattern ﾜｰｸ half_width_convert_comment BOOL\n",
      "checkpattern ﾜｰｸ half_width_convert_comment 出口STﾜｰｸ有\n",
      "checkpattern あり half_width_convert_comment GlobalVars\n",
      "checkpattern あり half_width_convert_comment BOOL\n",
      "checkpattern あり half_width_convert_comment 出口STﾜｰｸ有\n",
      "checkpattern 有 half_width_convert_comment GlobalVars\n",
      "checkpattern 有 half_width_convert_comment BOOL\n",
      "checkpattern 有 half_width_convert_comment 出口STﾜｰｸ有\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "workpiece_comment = \"ワーク\"\n",
    "with_comment_1 = \"あり\"\n",
    "with_comment_2 = \"有\"\n",
    "confirm_comment = \"確認\"\n",
    "contact_comment = ['GlobalVars', 'BOOL', '出口STﾜｰｸ有']\n",
    "# regex_pattern_check(workpiece_comment, a)\n",
    "if regex_pattern_check(workpiece_comment, contact_comment) and (regex_pattern_check(with_comment_1, contact_comment) or regex_pattern_check(with_comment_2, contact_comment) or regex_pattern_check(confirm_comment, contact_comment)):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12152b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_to_half_conversion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m condition_comment \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m条件\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m contact_comment \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHOME POSI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_原位置\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobalVars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m自動起動条件1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARRAY[1..15] OF BOOL\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mregex_pattern_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauto_comment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontact_comment\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m regex_pattern_check(start_comment, contact_comment) \u001b[38;5;129;01mand\u001b[39;00m regex_pattern_check(condition_comment, contact_comment) \u001b[38;5;129;01mand\u001b[39;00m regex_pattern_check(home_comment, contact_comment) \u001b[38;5;129;01mand\u001b[39;00m regex_pattern_check(pos_comment, contact_comment):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m, in \u001b[0;36mregex_pattern_check\u001b[1;34m(check_pattern, comment_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m ret_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_pattern \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(check_pattern, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m comment_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(comment_list, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     check_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_to_half_conversion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheck_pattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m comment_list:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m comment \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(comment, \u001b[38;5;28mstr\u001b[39m):\n",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m ret_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_pattern \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(check_pattern, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m comment_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(comment_list, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     check_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mfull_to_half_conversion\u001b[49m\u001b[38;5;241m.\u001b[39mget(char, char) \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m check_pattern)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m comment_list:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m comment \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(comment, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_to_half_conversion' is not defined"
     ]
    }
   ],
   "source": [
    "home_comment = \"原\"\n",
    "pos_comment = \"位置\"\n",
    "auto_comment = \"自動\"   \n",
    "start_comment = \"起動\"\n",
    "condition_comment = \"条件\"\n",
    "contact_comment = ['HOME POSI', '01_原位置', 'GlobalVars', '自動起動条件1', 'ARRAY[1..15] OF BOOL']\n",
    "if regex_pattern_check(auto_comment, contact_comment) and regex_pattern_check(start_comment, contact_comment) and regex_pattern_check(condition_comment, contact_comment) and regex_pattern_check(home_comment, contact_comment) and regex_pattern_check(pos_comment, contact_comment):\n",
    "    print(\"yes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "channel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
